{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size: 27\n",
      "32033\n",
      "15\n",
      "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n",
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"
     ]
    }
   ],
   "source": [
    "# Become a backprop ninja!\n",
    "# Making out MLP more understandable to us by replacing loss.backward() by our own expressions.\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# read all words from the baby names data\n",
    "words = open('names.txt','r').read().splitlines()\n",
    "\n",
    "# Now extract all the unique characters from the words\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "# convert characters to a dictionary with indices. This is our tokenizer\n",
    "char2idx = {c:i+1 for i,c in enumerate(chars)}\n",
    "# add a new index\n",
    "char2idx['.'] = 0\n",
    "# reverse the dictionary for referencing indices back to characters\n",
    "idx2char = {i:c for c,i in char2idx.items()}\n",
    "\n",
    "vocab_size = len(idx2char) # add one extra character for including the '.' we use to denote the beginning and the end of a word\n",
    "print('vocab_size:', vocab_size)\n",
    "\n",
    "# read in all the words\n",
    "print(len(words))\n",
    "print(max(len(w) for w in words))\n",
    "print(words[:8])\n",
    "print(idx2char)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182455, 3]) torch.Size([182455])\n",
      "torch.Size([22929, 3]) torch.Size([22929])\n",
      "torch.Size([22762, 3]) torch.Size([22762])\n"
     ]
    }
   ],
   "source": [
    "# Build the dataset consisting of first n letters in a word, and the prediction being the subsequent letter\n",
    "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "\n",
    "def build_dataset(words):  \n",
    "  X, Y = [], []\n",
    "  \n",
    "  for w in words:\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "      ix = char2idx[ch]\n",
    "      X.append(context)\n",
    "      Y.append(ix)\n",
    "      #print(''.join(idx2char[i] for i in context), '---->', idx2char[ix])\n",
    "      context = context[1:] + [ix] # crop and append\n",
    "\n",
    "  X = torch.tensor(X)\n",
    "  Y = torch.tensor(Y)\n",
    "  print(X.shape, Y.shape)\n",
    "  return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "X_train,Y_train = build_dataset(words[:n1])\n",
    "X_val,Y_val = build_dataset(words[n1:n2])\n",
    "X_test,Y_test = build_dataset(words[n2:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us now define a utility function called compare that will take in the gradients calculated\n",
    "# by us during the backprop to the actual gradient estimated by PyTorch\n",
    "\n",
    "def compare(s,dt,t):\n",
    "    ex = torch.all(dt==t.grad).item() # pytorch gradients\n",
    "    app = torch.allclose(dt,t.grad) # ours\n",
    "    maxdiff = torch.max(torch.abs(dt-t.grad)).item() # difference\n",
    "    print(f'Comparing {s:15s} | Exact {str(ex):5s} | Approx {str(app):5s} | Max Diff {maxdiff}') # verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4137\n"
     ]
    }
   ],
   "source": [
    "# Build and initialize a two layer MLP\n",
    "\n",
    "n_embed = 10\n",
    "n_hidden = 64\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # use the same as Karpathy's\n",
    "C = torch.randn((vocab_size,n_embed)                ,generator=g) # embedding matrix that converts words to embeddings\n",
    "# Layer 1\n",
    "W1 = torch.randn((n_embed*block_size,n_hidden)      ,generator=g) * (5/3)/(n_embed*n_hidden)**0.5 # initialize with reasonable weights\n",
    "b1 = torch.randn((n_hidden,)                        ,generator=g) * 0.1 # initialize with reasonable weights\n",
    "# Layer 2\n",
    "W2 = torch.randn((n_hidden,vocab_size)              ,generator=g) * 0.1 # initialize with reasonable weights\n",
    "b2 = torch.randn((vocab_size,)                      ,generator=g) * 0.1 # initialize with reasonable weights\n",
    "# Batchnorm between Layer 1 and Layer 2. This is why the weights of Layer 2 are initialized with 0.1 and not (5/3)/n_elem**0.5\n",
    "bngain = torch.randn((1,n_hidden)                   ,generator=g) * 0.1 +1.0 # initialize with reasonable weights and add 1 to make it non-zero\n",
    "bnbias = torch.randn((1,n_hidden)                   ,generator=g) * 0.1 # initialize with reasonable weights\n",
    "\n",
    "# According to Karpathy, the initialization multiples are non standard to avoid zero initialization\n",
    "# from masking any implementation errors.\n",
    "\n",
    "\n",
    "# concatenate all the above parameters to a parameter vector\n",
    "parameters = [C,W1,b1,W2,b2,bngain,bnbias]\n",
    "print(sum(p.nelement() for p in parameters))\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Input the batch_size and construct a mini batch of data for testing\n",
    "batch_size = 32\n",
    "# mini batch construction\n",
    "ix = torch.randint(0,X_train.shape[0],(batch_size,),generator=g)\n",
    "Xb = X_train[ix]\n",
    "Yb = Y_train[ix]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.7374, 3.9579, 3.0283, 3.2829, 2.6867, 3.1848, 3.4475, 2.9051, 3.7346,\n",
       "         3.6452, 2.9096, 2.9867, 2.2812, 2.7768, 3.9530, 2.7132, 2.1172, 2.0165,\n",
       "         3.7259, 2.4804, 2.6338, 4.6553, 3.8318, 3.7027, 3.4099, 2.7369, 3.4671,\n",
       "         4.3016, 3.6129, 2.9533, 3.0974, 2.8888, 3.1185, 2.8723, 2.7913, 2.9443,\n",
       "         2.9573, 2.9098, 7.6401, 2.2044, 3.3811, 2.6304, 4.2618, 2.9306, 3.6757,\n",
       "         2.3770, 3.8771, 2.7004, 3.8568, 2.7137, 2.5998, 3.0946, 2.1633, 3.1413,\n",
       "         2.3713, 2.7411, 4.0249, 2.4877, 2.4584, 4.8540, 3.1354, 2.7956, 3.2154,\n",
       "         2.8378]], grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Forward passing through the network\n",
    "\n",
    "n = batch_size\n",
    "# Embedding layer\n",
    "Xb_embed = C[Xb] # convert the input to embeddings\n",
    "# concatenate the embeddings\n",
    "Xb_concat = Xb_embed.view(Xb_embed.shape[0],-1)\n",
    "# Linear Layer 1\n",
    "hprebn = Xb_concat @ W1 + b1 # hidden layer preactivation\n",
    "# Batchnorm\n",
    "bnmeani = (1/n)*hprebn.sum(0,keepdim=True) # mean of the hidden layer preactivation\n",
    "bndiff = hprebn-bnmeani # difference of the hidden layer preactivation from the mean\n",
    "bnsqdiff = bndiff**2 # square of the difference\n",
    "bnvar = (1/(n-1))*bnsqdiff.sum(0,keepdim=True) # variance of the hidden layer preactivation\n",
    "\n",
    "##### digression to Bessel's correction #####\n",
    "# in batchnorm1d, the variance is said to be calculated as 1/n * sum((x_i - mean)**2)\n",
    "# but in actuality, it is 1/(n-1) * sum((x_i - mean)**2)\n",
    "# even if there is a flag for unbiased/biased in torch.var,\n",
    "# the unbiased is used as default.\n",
    "# therefore we should use 1/(n-1) instead of 1/n\n",
    "\n",
    "# the paper suggests using 1/n during training and 1/(n-1) during testing,\n",
    "# but karpathy prefers using 1/(n-1) during training and testing as well.\n",
    "\n",
    "\n",
    "\n",
    "# in the original paper, 1/n is used in the paper, however, this should be 1/(n-1) \n",
    "bnvar_inv = (bnvar + 1e-5)**-0.5 # inverse of the variance with 1e-5 to prevent division by zero\n",
    "bnraw = bndiff*bnvar_inv # raw output of the batchnorm\n",
    "hpreact = bngain*bnraw + bnbias # hidden layer preactivation\n",
    "# Non-linearity\n",
    "h = torch.tanh(hpreact)\n",
    "# Linear Layer 2\n",
    "logits = h @ W2 + b2 # logits\n",
    "# Cross entropy loss\n",
    "logits_maxes = logits.max(1,keepdim=True).values # max of the logits\n",
    "norm_logits = logits - logits_maxes # normalized logits by removing the max for numerical stability alone\n",
    "counts = norm_logits.exp() # convert the logits into a fake count like thing. \n",
    "counts_sum = counts.sum(1,keepdim=True) # sum of the counts\n",
    "counts_sum_inv = counts_sum**-1 # inverse of the sum of the counts\n",
    "probs = counts * counts_sum_inv\n",
    "logprobs = probs.log() # log of the probabilities\n",
    "loss = -logprobs[range(n),Yb].mean() # cross entropy loss\n",
    "\n",
    "\n",
    "# Pytorch backward pass\n",
    "for p in parameters:\n",
    "    p.grad = None\n",
    "for t in [logprobs,probs,counts,counts_sum,counts_sum_inv,\n",
    "            norm_logits,logits_maxes,logits,h,hpreact,\n",
    "            bnraw,bnvar_inv,bndiff,bnsqdiff,bnvar,bnmeani,\n",
    "            hprebn,Xb_concat,Xb_embed]:\n",
    "        \n",
    "    t.retain_grad()\n",
    "loss.backward()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing logprobs        | Exact True  | Approx True  | Max Diff 0.0\n",
      "Comparing probs           | Exact True  | Approx True  | Max Diff 0.0\n",
      "Comparing counts_sum_inv  | Exact True  | Approx True  | Max Diff 0.0\n",
      "Comparing counts_sum      | Exact True  | Approx True  | Max Diff 0.0\n",
      "Comparing counts          | Exact True  | Approx True  | Max Diff 0.0\n",
      "Comparing norm_logits     | Exact True  | Approx True  | Max Diff 0.0\n",
      "Comparing logits_maxes    | Exact True  | Approx True  | Max Diff 0.0\n",
      "Comparing logits before logits_maxes is corrected | Exact False | Approx True  | Max Diff 4.6566128730773926e-09\n",
      "Comparing logits after logits_maxes is corrected | Exact True  | Approx True  | Max Diff 0.0\n",
      "Comparing h               | Exact True  | Approx True  | Max Diff 0.0\n",
      "Comparing W2              | Exact True  | Approx True  | Max Diff 0.0\n",
      "Comparing b2              | Exact True  | Approx True  | Max Diff 0.0\n",
      "Comparing hpreact         | Exact True  | Approx True  | Max Diff 0.0\n",
      "Comparing bngain          | Exact True  | Approx True  | Max Diff 0.0\n",
      "Comparing bnbias          | Exact True  | Approx True  | Max Diff 0.0\n",
      "Comparing bnraw           | Exact True  | Approx True  | Max Diff 0.0\n",
      "Comparing bndiff before correction | Exact False | Approx False | Max Diff 0.006515678483992815\n",
      "Comparing bnvarinv        | Exact True  | Approx True  | Max Diff 0.0\n",
      "Comparing bnvar           | Exact True  | Approx True  | Max Diff 0.0\n",
      "Comparing bnsqdiff        | Exact True  | Approx True  | Max Diff 0.0\n",
      "Comparing bndiff after correction | Exact True  | Approx True  | Max Diff 0.0\n",
      "Comparing bnmeani         | Exact True  | Approx True  | Max Diff 0.0\n",
      "Comparing hprebn before correction | Exact False | Approx False | Max Diff 0.007046092301607132\n",
      "Comparing hprebn after correction | Exact True  | Approx True  | Max Diff 0.0\n",
      "Comparing Xb_concat       | Exact True  | Approx True  | Max Diff 0.0\n",
      "Comparing W1              | Exact True  | Approx True  | Max Diff 0.0\n",
      "Comparing b1              | Exact True  | Approx True  | Max Diff 0.0\n",
      "Comparing Xb_embed        | Exact True  | Approx True  | Max Diff 0.0\n",
      "Comparing C               | Exact True  | Approx True  | Max Diff 0.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Here we implement the derivative of each expression in the layers of the 2-layer MLP\n",
    "\n",
    "# dlogprobs/dloss\n",
    "dlogprobs = torch.zeros_like(logprobs)\n",
    "dlogprobs[range(n),Yb] = -1.0/n    # d loss / d logprobs = that of a row in logprobs whose mean is taken to get loss.\n",
    "compare('logprobs',dlogprobs,logprobs)\n",
    "\n",
    "# dprobs/dlogprobs = d (log(probs))/dprobs = 1/probs\n",
    "dprobs = 1.0/probs * dlogprobs # also multiply the derivative carries backwards (dlogprobs/dloss)\n",
    "compare('probs',dprobs,probs)\n",
    "\n",
    "# dcounts_sum_inv/dprobs = d (counts*counts_sum_inv)/dprobs = d (counts * counts_sum**-1)/dprobs \n",
    "# On a basic level, the following happens:\n",
    "# logits = xenc@W\n",
    "# counts = logits.exp()\n",
    "# probs = counts / counts.sum(1, keepdim=True)\n",
    "# Also, counts is (batch_size, vocab_size) and counts_sum_inv is (batch_size, 1) so there is an implicit broadcasting that pytorch does\n",
    "# This means that before the multiplication, a copying of the cons_sum_inv across the vocab_size dimension happens, therefore,\n",
    "# the probs operation is actually two separate operations.\n",
    "# Let us first backprop through the multiplication\n",
    "# dcounts_sum_inv = (counts * dprobs) # this is the derivative of the replication, which need to be summed over the vocab_size dimension\n",
    "# to complete the operation\n",
    "dcounts_sum_inv = (counts * dprobs).sum(1,keepdim=True) # this is the derivative of the sum\n",
    "compare('counts_sum_inv',dcounts_sum_inv,counts_sum_inv)\n",
    "# dcounts_sum\n",
    "dcounts_sum = (-counts_sum**-2) * dcounts_sum_inv # this is the derivative of the square\n",
    "compare('counts_sum',dcounts_sum,counts_sum)\n",
    "# dcounts_sum/dcounts_sum_inv = \n",
    "dcounts = counts_sum_inv * dprobs \n",
    "# This needs better understanding with pen and paper, but as counts is used twice, the derivatives w.r.t separate functions need to be added\n",
    "dcounts += torch.ones_like(counts) * dcounts_sum  # as dcounts was previously already added, we need to add this to the previous derivative\n",
    "compare('counts',dcounts,counts)\n",
    "\n",
    "# dcounts/dnorm_logits = d(exp(norm_logits))/dnorm_logits = exp(norm_logits)\n",
    "dnorm_logits = norm_logits.exp() * dcounts\n",
    "compare('norm_logits',dnorm_logits,norm_logits)\n",
    "\n",
    "# norm_logits = logits - logits_maxes; \n",
    "# Can be written in an example form as the following:\n",
    "# c11 c12 c13 = a11 a12 a13 - b1 \n",
    "# c21 c22 c23 = a21 a22 a23 - b2\n",
    "# c31 c32 c33 = a31 a32 a33 - b3\n",
    "# therefore, c23 = a23 - b2  so d c23 / d a23 = 1 and d c23 / d b2 = -1; remember that b is implicitly broadcasted because it has a smaller dimension\n",
    "# if dlogits is the a matrix and the dlogits_maxes is the b matrix, then the following is the derivative\n",
    "dlogits = torch.ones_like(logits) * dnorm_logits # or dnorm_logits.clone()\n",
    "dlogits_maxes = -dnorm_logits.sum(1,keepdim=True) # where summing is performed because of the implicit broadcasting\n",
    "compare('logits_maxes',dlogits_maxes,logits_maxes)\n",
    "compare('logits before logits_maxes is corrected',dlogits,logits) # This will be approximately true, but not quite exactly, because logits_maxes is itself a function of logits\n",
    "\n",
    "# One point to note is that norm_logits = logits - logits_maxes, the subtraction is done to prevent overflow in the exp function, i.e., very high values\n",
    "# norm_logits is making sure that the highest number that goes to the exp function is 0.\n",
    "# Therefore dlogits_maxes will be in general very small numbers because changing logits maxes will not change probs much.\n",
    "\n",
    "# logits_maxes = logits.max(1,keepdim=True).values\n",
    "logits.max(1,keepdim=True) # This stores both the maxes and the indices of the maxes\n",
    "# In the forward pass, the indices are not used. We only need the maxes.\n",
    "# but in the backward pass, we need the indices to know which element of the logits to subtract from.\n",
    "# Therefore the local derivative is 1 at the index the max was plucked out from and zero everywhere else.\n",
    "# This can be done using the one_hot function\n",
    "dlogits += F.one_hot((logits.max(1).indices),num_classes = logits.shape[1]) * dlogits_maxes \n",
    "# remember that dlogits_maxes is broadcasted into a shape the one_hot matrix can be multiplied with.\n",
    "# This means the dlogits_maxes will be multiplied to all elements of the one_hot matrix, but since\n",
    "# the place where the max does not exist is zero, the derivative will not be carried to that place.\n",
    "# It will only be carried to the index which is 1, which corresponds to the max value.\n",
    "# Now let us compare:\n",
    "compare('logits after logits_maxes is corrected',dlogits,logits)\n",
    "# visualizing the one hot function\n",
    "plt.imshow(F.one_hot((logits.max(1).indices),num_classes = logits.shape[1])) \n",
    "# This shows where the max was plucked out from. The rest of the matrix is zero. This is exactly what we need\n",
    "# because the one_hot is one at the needed site, given by the indices, and zero everywhere else.\n",
    "\n",
    "# Now, logits = h @ W2 + b2\n",
    "dlogits.shape, h.shape, W2.shape, (h@W2).shape,b2.shape \n",
    "# gives (32,27), (32,64), (64,27), (32,27),(27,), so the + operator for the b2 bias is broadcasted 32 times\n",
    "\n",
    "# now let us take a small example to understand this logits calculation better\n",
    "# d11 d12 = a11 a12   b11 b12    c1 c2\n",
    "#                   *          +\n",
    "# d21 d22 = a21 a22   b21 b22    c1 c2\n",
    "# c corresponds to the broadcased b2, b1 is the W2 matrix and h is the a matrix\n",
    "# This gives\n",
    "# d11 = (a11 * b11) + (a12 * b21) + c1\n",
    "# d12 = (a11 * b12) + (a12 * b22) + c2         # exp_1\n",
    "# d21 = (a21 * b11) + (a22 * b21) + c1\n",
    "# d22 = (a21 * b12) + (a22 * b22) + c2\n",
    "# We have the derivative of the loss w.r.t. d, we need to know the derivative of the loss w.r.t. a, b and c\n",
    "# dloss/dd = dloss/dd * dd/da * dd/db * dd/dc\n",
    "# dloss/da11 = dloss/dd11 * b11 + dloss/dd12 * b12 (all others are zero because d21 and d22 do not have a11)\n",
    "# Similarly,\n",
    "# dloss/da12 = dloss/dd11 * b21 + dloss/dd12 * b22 (all others are zero because d21 and d22 do not have a12)\n",
    "# dloss/da21 = dloss/dd21 * b11 + dloss/dd22 * b12 (all others are zero because d11 and d12 do not have a21)\n",
    "# dloss/da22 = dloss/dd21 * b21 + dloss/dd22 * b22 (all others are zero because d11 and d12 do not have a22)\n",
    "# Therefore:\n",
    "# dloss/da = dloss/da11 dloss/da12\n",
    "#            dloss/da21 dloss/da22\n",
    "#       dloss/da11 dloss/da12           dloss/dd11 dloss/dd12   b11 b21\n",
    "# =>                              =                           * \n",
    "#       dloss/da21 dloss/da22           dloss/dd21 dloss/dd22   b12 b22\n",
    "# Note how the b matrix is now transposed. \n",
    "# Therefore, dloss/da = dloss/dd @ b.T\n",
    "\n",
    "# dloss/db is tedious, but you could take the derivative of dloss/db and it should be something like:\n",
    "# dloss/db = a.T @ dloss/dd \n",
    "\n",
    "# dloss/dc1 = dloss/dd11 * 1 + dloss/dd21 * 1\n",
    "# dloss/dc2 = dloss/dd12 * 1 + dloss/dd22 * 1\n",
    "# Therefore dloss/dc = dloss/dd summed over the columns\n",
    "# dloss/dc = dloss/dd.sum(0)\n",
    "\n",
    "# Therefore, the following is the derivative of the loss w.r.t. the logits\n",
    "dh = dlogits @ W2.T # dloss/da = dloss/dd @ b.T\n",
    "dW2 = h.T @ dlogits # dloss/db = a.T @ dloss/dd\n",
    "db2 = dlogits.sum(0) # dloss/dc = dloss/dd.sum(0)\n",
    "# perform the comparison\n",
    "compare('h',dh,h)\n",
    "compare('W2',dW2,W2)\n",
    "compare('b2',db2,b2)\n",
    "# It is interesting to note that in the micrograd, we did not transpose, we just exchanged the gradients\n",
    "# However, when you have a matrix, the transpose is needed. This makes sense because otherwise,\n",
    "# the matrix multiplication is wrongly sized. \n",
    "\n",
    "# dhpreact is needed, which is backpropagation through a tanh function\n",
    "# remember that tanh derivation is as follows:\n",
    "# if a = tanh(x), then da/dx = 1 - a^2 = 1 - tanh(x)^2\n",
    "# therefore, dhpreact = dh * (1 - h^2)\n",
    "dhpreact = dh * (1 - h**2)\n",
    "compare('hpreact',dhpreact,hpreact)\n",
    "\n",
    "# We need to backpropagate through the batchnorm layer:\n",
    "# hpreact = bngain*bnraw + bnbias # hidden layer preactivation\n",
    "# While this looks like logits = h @ W2 + b2,\n",
    "# this isn't a matrix multiplication, rather it is an element wise multiplication.\n",
    "# this you can observe by looking at the size of bngain and bnbias\n",
    "hpreact.shape, bngain.shape, bnraw.shape, bnbias.shape\n",
    "# If you look at the shape differences, clearly bngain and bnbias are being broadcasted\n",
    "# in order to multiply with bnraw.\n",
    "dbngain = (dhpreact * bnraw).sum(0) # dloss/dbngain = dloss/dhpreact * [dhpreact/dbngain ; which is bnraw] (chain rule) \n",
    "# the summing is done because of the broadcasting, which means summing over the columns of (dhpreact * bnraw)\n",
    "dbnbias = dhpreact.sum(0) # dloss/dbnbias = dloss/dhpreact * [dhpreact/dbnbias which is 1]; (chain rule) \n",
    "# the summing is done because of the broadcasting, which means summing over the columns of dhpreact\n",
    "# Let us now compare with pytorch\n",
    "compare('bngain',dbngain,bngain)\n",
    "compare('bnbias',dbnbias,bnbias)\n",
    "dbnbias.shape, dbngain.shape # these are the wrong shapes, we need to keep the dim\n",
    "# rewriting the above\n",
    "dbngain = (dhpreact * bnraw).sum(0,keepdim=True) # dloss/dbngain = dloss/dhpreact * [dhpreact/dbngain ; which is bnraw] (chain rule)\n",
    "dbnbias = dhpreact.sum(0,keepdim=True) # dloss/dbnbias = dloss/dhpreact * [dhpreact/dbnbias which is 1]; (chain rule)\n",
    "dbnbias.shape, dbngain.shape # these are the right shapes\n",
    "\n",
    "# dbnraw is needed, which is backpropagation through the batchnorm layer\n",
    "dbnraw = dhpreact * bngain # no broadcasting done here and therefore, there is no column wise addition\n",
    "dbnraw.shape\n",
    "# compare\n",
    "compare('bnraw',dbnraw,bnraw)\n",
    "\n",
    "# now, bnraw = bndiff * bnvarinv \n",
    "# so dloss/dbndiff = dloss/dbnraw * dbnraw/dbndiff \n",
    "# and dloss/dbnvar =  dloss/dbnraw * dbnraw/dbnvarinv\n",
    "dbndiff = (dbnraw * bnvar_inv) # dloss/dbndiff = dloss/dbnraw * dbnraw/dbndiff\n",
    "dbnvarinv = (dbnraw * bndiff).sum(0,keepdim=True) # dloss/dbnvar =  dloss/dbnraw * dbnraw/dbnvarinv, summing is to respect broadcasting\n",
    "# compare\n",
    "compare('bndiff before correction',dbndiff,bndiff) \n",
    "compare('bnvarinv',dbnvarinv,bnvar_inv)\n",
    "\n",
    "# bndiff expected to be incorrect because bnvar_inv is a function of bnvar, which is a function of bndiff2, which is again a function of bndiff\n",
    "# therefore, some additions are necessary for bndiff. We will leave them be for now and continue. When we encounter\n",
    "# bndiff again we will simply += to the previous value. This should make the gradient correct.\n",
    "\n",
    "# Continuing\n",
    "# bnvar_inv = (bnvar + 1e-5)**-0.5 # inverse of the variance with 1e-5 to prevent division by zero\n",
    "dbnvar = dbnvarinv * (-0.5 * (bnvar + 1e-5)**-1.5)\n",
    "compare('bnvar',dbnvar,bnvar)\n",
    "\n",
    "# bnvar = (1/(n-1)) * bnsqdiff.sum(0,keepdim=True) # variance\n",
    "# because there is summing, the derivative is (1/(n-1))\n",
    "dbnsqdiff = (dbnvar * (1/(n-1))*torch.ones_like(bnsqdiff)) # dloss/dbsqndiff = dloss/dbnvar * dbnvar/dbnsqdiff\n",
    "compare('bnsqdiff',dbnsqdiff,bnsqdiff)\n",
    "\n",
    "# bnsqdiff = bndiff**2 # square of the difference\n",
    "dbndiff += dbnsqdiff * (2 * bndiff) # dloss/dbndiff = dloss/dbnsqdiff * dbnsqdiff/dbndiff\n",
    "compare('bndiff after correction',dbndiff,bndiff) # now it should be correct\n",
    "\n",
    "\n",
    "# bndiff = hprebn-bnmeani # difference of the hidden layer preactivation from the mean\n",
    "bndiff.shape, bnmeani.shape, hprebn.shape\n",
    "# The above give (32,64) (1,64) (32,64) which means that bnmeani is being broadcasted for the minus sign\n",
    "# Broadcasting means variable reuse, which means a sum in the backward pass.\n",
    "dhprebn = dbndiff # dloss/dhprebn = dloss/dbndiff * dbndiff/dhprebn = dloss/dbndiff * 1\n",
    "dbnmeani = (-torch.ones_like(bndiff) * dbndiff).sum(0,keepdim=True)   # dloss/dbnmeani = dloss/dbndiff * dbndiff/dbnmeani = dloss/dbndiff * -1, the summing is to allow broadcasting\n",
    "compare('bnmeani',dbnmeani,bnmeani)\n",
    "compare('hprebn before correction',dhprebn,hprebn) \n",
    "\n",
    "# dhprebn is calculated wrongly as we are bnmeani is a function of hprebn\n",
    "# therefore, we need to add the derivative of bnmeani with respect to hprebn\n",
    "\n",
    "# bnmeani = 1/n*hprebn.sum(0,keepdim=True) # mean of the hidden layer preactivation\n",
    "dhprebn += torch.ones_like(hprebn)*(dbnmeani * (1.0/n)) # dloss/dhprebn = dloss/dbnmeani * dbnmeani/dhprebn = dloss/dbnmeani * 1/n\n",
    "# torch.ones_like is used here because of the broadcasting\n",
    "compare('hprebn after correction',dhprebn,hprebn) \n",
    "\n",
    "# now hprebn = Xb_concat @ W1 + b1 \n",
    "# so going by our previous treatment of the linear layer\n",
    "dXb_concat = dhprebn @ W1.T # dloss/dXb_concat = dloss/dhprebn * dhprebn/dXb_concat = dloss/dhprebn * W1.T\n",
    "dW1 = Xb_concat.T @ dhprebn # dloss/dW1 = dloss/dhprebn * dhprebn/dW1 = dloss/dhprebn * Xb_concat.T\n",
    "db1 = dhprebn.sum(0,keepdim=True) # dloss/db1 = dloss/dhprebn * dhprebn/db1 = dloss/dhprebn * 1, summing is to allow broadcasting\n",
    "compare('Xb_concat',dXb_concat,Xb_concat)\n",
    "compare('W1',dW1,W1)\n",
    "compare('b1',db1,b1)\n",
    "\n",
    "\n",
    "# Xb_concat = Xb_embed.view(Xb_embed.shape[0],-1)\n",
    "# Xb_concat is a concatenated version of Xb_embed\n",
    "# Therefore we have to undo the concatenation for the backward pass\n",
    "dXb_embed= dXb_concat.view(Xb_embed.shape)\n",
    "compare('Xb_embed',dXb_embed,Xb_embed)\n",
    "\n",
    "# Xb_embed = C[Xb] # convert the input to embeddings\n",
    "# This operation just pulls out the rows of the embedding matrix, \n",
    "# which means we need a one-hot encoding to route the gradient to the right place\n",
    "# This cannot be done in a more efficient way besides using for-loops\n",
    "dC = torch.zeros_like(C)\n",
    "for k in range(Xb.shape[0]):\n",
    "    for j in range(Xb.shape[1]):\n",
    "        ix = Xb[k,j]\n",
    "        dC[ix] += dXb_embed[k,j] # addition because you may find the same index multiple times\n",
    "compare('C',dC,C)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing logits          | Exact False | Approx True  | Max Diff 4.423782229423523e-09\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(6.9849e-10, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# While instructive, the above backpropagation is not very efficient.\n",
    "# all of the expressions had to be unpacked to make it manageble to understand.\n",
    "\n",
    "# dlogits can be calculated much more simply if one looks at the full expression\n",
    "# on pen and paper. Not doing it here.\n",
    "\n",
    "dlogits = F.softmax(logits, 1) # apply logits to the rows of logits\n",
    "dlogits[range(n),Yb] -= 1 # subtract 1 from the correct class\n",
    "dlogits /= n # divide by the number of samples\n",
    "compare('logits',dlogits,logits)\n",
    "# You do not get an exact match, but the difference is very small.\n",
    "# This is due to floating point wonkiness.\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.imshow(dlogits.detach(),cmap='gray')\n",
    "\n",
    "# in a forward pass, logits is the probability matrix\n",
    "# If you see the plot below, the black square are the correct classes where we \n",
    "# subtracted 1, which means they are zero, exp(0) is 1, and the softmax is 1, i.e., the correct class\n",
    "# take for example \n",
    "F.softmax(logits,1)[0] # the first row of logits\n",
    "# and look at the backward pass\n",
    "dlogits[0]*n # the first row of dlogits\n",
    "# it is -1 in one spot and above zero on the others\n",
    "# Furthermore, the sum of this is very small\n",
    "(dlogits[0]).sum()\n",
    "# Therefore, the gradients at each cell of dlogits\n",
    "# tries to pull down the probability of the incorrect characters/class\n",
    "# while it tries to pull up the probability of the correct character/class\n",
    "# The amount of push and pull is equalized, as their row sum is zero.\n",
    "\n",
    "# What that means is that currently in the forward pass the prediction is wrong\n",
    "# therefore dlogits is set to negative of the position at that predicted character\n",
    "# and is trying to move the network away from that prediction. They are shown as black in the \n",
    "# plot because they are negative, and the implot cannot show negative values.\n",
    "\n",
    "# Therefore, one can consider the nn as a pulley system which tries to pull the correct class up\n",
    "# and the incorrect classes down, while keeping the forces balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max diff: tensor(4.7684e-07, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# we will now also try to glue the batchnorm expressions together for faster forward pass\n",
    "\n",
    "hpreact_fast = bngain * (hprebn - hprebn.mean(0,keepdim=True)) / torch.sqrt(hprebn.var(0,keepdim=True,unbiased=True) + 1e-5) + bnbias\n",
    "print('max diff:', (hpreact_fast - hpreact).abs().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing hprebn fast version | Exact False | Approx True  | Max Diff 5.587935447692871e-09\n"
     ]
    }
   ],
   "source": [
    "# Let us now try to implement a fast backward pass using the derivative of the fused expression\n",
    "# which offers the same benefits of cancelling of terms as seen with dlogits (fast version)\n",
    "\n",
    "dhprebn = bngain * bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))\n",
    "compare('hprebn fast version',dhprebn,hprebn)\n",
    "\n",
    "# Not exactly the same but the difference is very small due to floating point wonkiness.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12297\n",
      "      0/ 200000: 3.5426\n",
      "  10000/ 200000: 2.4924\n",
      "  20000/ 200000: 2.3489\n",
      "  30000/ 200000: 2.5809\n",
      "  40000/ 200000: 2.0048\n",
      "  50000/ 200000: 2.0849\n",
      "  60000/ 200000: 2.4541\n",
      "  70000/ 200000: 2.2546\n",
      "  80000/ 200000: 1.8467\n",
      "  90000/ 200000: 2.1783\n"
     ]
    }
   ],
   "source": [
    "# putting it all together\n",
    "\n",
    "# Build and initialize a two layer MLP\n",
    "\n",
    "from tqdm import trange\n",
    "\n",
    "n_embed = 10\n",
    "n_hidden = 200\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # use the same as Karpathy's\n",
    "C = torch.randn((vocab_size,n_embed)                ,generator=g) # embedding matrix that converts words to embeddings\n",
    "# Layer 1\n",
    "W1 = torch.randn((n_embed*block_size,n_hidden)      ,generator=g) * (5/3)/((n_embed * block_size)**0.5) # initialize with reasonable weights\n",
    "b1 = torch.randn((n_hidden,)                        ,generator=g) * 0.1 # initialize with reasonable weights\n",
    "# Layer 2\n",
    "W2 = torch.randn((n_hidden,vocab_size)              ,generator=g) * 0.1 # initialize with reasonable weights\n",
    "b2 = torch.randn((vocab_size,)                      ,generator=g) * 0.1 # initialize with reasonable weights\n",
    "# Batchnorm between Layer 1 and Layer 2. This is why the weights of Layer 2 are initialized with 0.1 and not (5/3)/n_elem**0.5\n",
    "bngain = torch.randn((1,n_hidden)) * 0.1 +1.0 # initialize with reasonable weights and add 1 to make it non-zero\n",
    "bnbias = torch.randn((1,n_hidden)) * 0.1 # initialize with reasonable weights\n",
    "\n",
    "# According to Karpathy, the initialization multiples are non standard to avoid zero initialization\n",
    "# from masking any implementation errors.\n",
    "\n",
    "# concatenate all the above parameters to a parameter vector\n",
    "parameters = [C,W1,b1,W2,b2,bngain,bnbias]\n",
    "print(sum(p.nelement() for p in parameters))\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "# optimizer params\n",
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "n = batch_size  \n",
    "lossi = [] # intermediate loss for plotting\n",
    "with torch.no_grad(): # we do not need to track gradients for the optimizer as we have our own implementation\n",
    "    for i in range(max_steps):\n",
    "    #for i in trange(max_steps, desc=\"Training\", unit=\"it\"):\n",
    "\n",
    "        # mini batch construction\n",
    "        ix = torch.randint(0,X_train.shape[0],(batch_size,),generator=g)\n",
    "        Xb ,Yb = X_train[ix],Y_train[ix]\n",
    "        \n",
    "        \n",
    "        # Forward passing through the network\n",
    "        # Embedding layer\n",
    "        emb = C[Xb] # convert the input to embeddings\n",
    "        # concatenate the embeddings\n",
    "        embcat = emb.view(emb.shape[0],-1)\n",
    "        # Linear Layer 1\n",
    "        hprebn = embcat @ W1 + b1 # hidden layer preactivation\n",
    "        # Batchnorm\n",
    "        bnmean = hprebn.mean(0,keepdim=True) # mean of the hidden layer preactivation\n",
    "        bnvar = hprebn.var(0,keepdim=True,unbiased=True) # variance of the hidden layer preactivation\n",
    "        # Layer norm (do not update bnmean and bnvar as the averaging is not performed over the batches now)\n",
    "        #bnmean = hprebn.mean(1,keepdim=True) # mean of the hidden layer preactivation\n",
    "        #bnvar = hprebn.var(1,keepdim=True,unbiased=True) # variance of the hidden layer preactivation\n",
    "        bnvar_inv = (bnvar + 1e-5)**-0.5 # inverse of the variance with 1e-5 to prevent division by zero\n",
    "        bnraw = (hprebn - bnmean) * bnvar_inv # raw output of the batchnorm\n",
    "        hpreact = bngain * bnraw + bnbias\n",
    "        \n",
    "        # Non-linearity\n",
    "        h = torch.tanh(hpreact)\n",
    "        # Linear Layer 2\n",
    "        logits = h @ W2 + b2 # logits\n",
    "        # Cross entropy loss\n",
    "        loss = F.cross_entropy(logits,Yb) # cross entropy loss\n",
    "\n",
    "        # backward pass\n",
    "\n",
    "        # Pytorch backward pass\n",
    "        for p in parameters:\n",
    "            p.grad = None\n",
    "        #loss.backward()\n",
    "\n",
    "        # Your own backward pass:\n",
    "        # cross entropy backward pass\n",
    "        dlogits = F.softmax(logits, 1) # apply logits to the rows of logits\n",
    "        dlogits[range(n),Yb] -= 1 # subtract 1 from the correct class\n",
    "        dlogits /= n # divide by the number of samples\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Linear Layer 2 backward pass\n",
    "        dh = dlogits @ W2.T # dloss/da = dloss/dd @ b.T\n",
    "        dW2 = h.T @ dlogits # dloss/db = a.T @ dloss/dd\n",
    "        db2 = dlogits.sum(0) # dloss/dc = dloss/dd.sum(0)\n",
    "        # Non-linearity backward pass\n",
    "        dhpreact = (1.0 - h**2) * dh \n",
    "       \n",
    "        # Batchnorm backward pass\n",
    "        dbngain = (bnraw * dhpreact).sum(0,keepdim=True) # dloss/dbngain = dloss/dhpreact * [dhpreact/dbngain ; which is bnraw] (chain rule) \n",
    "        dbnbias = dhpreact.sum(0,keepdim=True) # dloss/dbnbias = dloss/dhpreact * [dhpreact/dbnbias ; which is 1] (chain rule)\n",
    "        dhprebn = bngain * bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))\n",
    "        \n",
    "        \n",
    "        # Linear Layer 1 backward pass\n",
    "        dembcat = dhprebn @ W1.T # dloss/dXb_concat = dloss/dhprebn * dhprebn/dXb_concat = dloss/dhprebn * W1.T\n",
    "        dW1 = embcat.T @ dhprebn # dloss/dW1 = dloss/dhprebn * dhprebn/dW1 = dloss/dhprebn * Xb_concat.T\n",
    "        db1 = dhprebn.sum(0) # dloss/db1 = dloss/dhprebn * dhprebn/db1 = dloss/dhprebn * 1, summing is to allow broadcasting\n",
    "        \n",
    "        \n",
    "        # Embedding backward pass\n",
    "        demb= dembcat.view(emb.shape)\n",
    "        dC = torch.zeros_like(C)\n",
    "        for k in range(Xb.shape[0]):\n",
    "            for j in range(Xb.shape[1]):\n",
    "                ix = Xb[k,j]\n",
    "                dC[ix] += demb[k,j] # addition because you may find the same index multiple times\n",
    "        \n",
    "\n",
    "        # Collect all the grads\n",
    "        grads = [dC, dW1, db1, dW2, db2, dbngain, dbnbias] \n",
    "        \n",
    "        # update the parameters\n",
    "        lr = 0.1 if i<100000 else 0.01\n",
    "        # Own method\n",
    "        for p,grad in zip(parameters,grads):\n",
    "            p.data += -lr * grad\n",
    "        \n",
    "        # track stats\n",
    "        if i % 10000 == 0: # print every once in a while\n",
    "            print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "            #print(f'training loss: {loss.item():.4f}')\n",
    "        lossi.append(loss.log10().item())\n",
    "        \n",
    "        \n",
    "plt.plot(lossi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calibrate the batch norm at the end of training (avoid if layer norm is used)\n",
    "\n",
    "with torch.no_grad():\n",
    "  # pass the training set through\n",
    "  emb = C[X_train]\n",
    "  embcat = emb.view(emb.shape[0], -1)\n",
    "  hpreact = embcat @ W1 + b1\n",
    "  # measure the mean/std over the entire training set\n",
    "  bnmean = hpreact.mean(0, keepdim=True)\n",
    "  bnvar = hpreact.var(0, keepdim=True, unbiased=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 2.0687193870544434\n",
      "val 2.116607666015625\n"
     ]
    }
   ],
   "source": [
    "# evaluate train and val loss\n",
    "\n",
    "@torch.no_grad() # this decorator disables gradient tracking\n",
    "def split_loss(split):\n",
    "  x,y = {\n",
    "    'train': (X_train, Y_train),\n",
    "    'val': (X_val, Y_val),\n",
    "    'test': (X_test, Y_test),\n",
    "  }[split]\n",
    "  emb = C[x] # (N, block_size, n_embd)\n",
    "  embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
    "  hpreact = embcat @ W1 + b1\n",
    "  hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n",
    "  h = torch.tanh(hpreact) # (N, n_hidden)\n",
    "  logits = h @ W2 + b2 # (N, vocab_size)\n",
    "  loss = F.cross_entropy(logits, y)\n",
    "  print(split, loss.item())\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "carmah.\n",
      "ambrie.\n",
      "khi.\n",
      "mili.\n",
      "taty.\n",
      "salani.\n",
      "emmahnee.\n",
      "delynn.\n",
      "jareen.\n",
      "ner.\n",
      "kiah.\n",
      "maiivon.\n",
      "leigh.\n",
      "ham.\n",
      "prin.\n",
      "quint.\n",
      "shon.\n",
      "walianni.\n",
      "waython.\n",
      "jaryn.\n"
     ]
    }
   ],
   "source": [
    "# sample from the model\n",
    "g = torch.Generator().manual_seed(2147483647 + 10)\n",
    "\n",
    "for _ in range(20):\n",
    "    \n",
    "    out = []\n",
    "    context = [0] * block_size # initialize with all ...\n",
    "    while True:\n",
    "      # ------------\n",
    "      # forward pass:\n",
    "      # Embedding\n",
    "      emb = C[torch.tensor([context])] # (1,block_size,d)      \n",
    "      embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
    "      hpreact = embcat @ W1 + b1\n",
    "      hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n",
    "      h = torch.tanh(hpreact) # (N, n_hidden)\n",
    "      logits = h @ W2 + b2 # (N, vocab_size)\n",
    "      # ------------\n",
    "      # Sample\n",
    "      probs = F.softmax(logits, dim=1)\n",
    "      ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "      context = context[1:] + [ix]\n",
    "      out.append(ix)\n",
    "      if ix == 0:\n",
    "        break\n",
    "    \n",
    "    print(''.join(idx2char[i] for i in out))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textscan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d98cbb787251fac8b09d58c88d44135973ceca7df75589457d3db0159f5eed1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
